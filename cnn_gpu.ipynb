{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP3ucUIjuhPDu3BlhE0zA/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergeychuvakin/random_scripts/blob/master/cnn_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlBApk1AW2-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95fb43e8-8973-49c5-cf5d-7775a1113e45"
      },
      "source": [
        "# то что долго нужно ждать (минут 10-15)\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\n",
        "!unzip wiki.en.zip\n",
        "!wget https://github.com/sergeychuvakin/random_scripts/raw/master/fin3.pickle\n",
        "!pip install fasttext\n",
        "!rm -rf wiki.en.zip\n",
        "!rm -rf wiki.en.vec\n",
        "\n",
        "import fasttext\n",
        "model_ft = fasttext.load_model('wiki.en.bin')\n",
        "\n",
        "import nltk \n",
        "nltk.download(\"all\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-14 09:22:58--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10356881291 (9.6G) [application/zip]\n",
            "Saving to: ‘wiki.en.zip’\n",
            "\n",
            "wiki.en.zip         100%[===================>]   9.65G  27.5MB/s    in 3m 50s  \n",
            "\n",
            "2020-08-14 09:26:49 (42.9 MB/s) - ‘wiki.en.zip’ saved [10356881291/10356881291]\n",
            "\n",
            "Archive:  wiki.en.zip\n",
            "  inflating: wiki.en.vec             \n",
            "  inflating: wiki.en.bin             \n",
            "--2020-08-14 09:33:23--  https://github.com/sergeychuvakin/random_scripts/raw/master/fin3.pickle\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sergeychuvakin/random_scripts/master/fin3.pickle [following]\n",
            "--2020-08-14 09:33:23--  https://raw.githubusercontent.com/sergeychuvakin/random_scripts/master/fin3.pickle\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26170530 (25M) [application/octet-stream]\n",
            "Saving to: ‘fin3.pickle’\n",
            "\n",
            "fin3.pickle         100%[===================>]  24.96M  57.7MB/s    in 0.4s    \n",
            "\n",
            "2020-08-14 09:33:24 (57.7 MB/s) - ‘fin3.pickle’ saved [26170530/26170530]\n",
            "\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (49.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3015553 sha256=2e82048ddb9b6d2e631391662ad299342da108a29689704b20e3cee9b5d5f4a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSN2tiPJwfEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxyGGgI6wh9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('fin3.pickle', 'rb') as f: \n",
        "    df = pickle.load(f)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23sH9K7Kwnjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5285348e-edfd-4c87-c843-fee7cb4b9bec"
      },
      "source": [
        "texts = df['text']\n",
        "labels = df['_class']\n",
        "word2idx = {}\n",
        "del df\n",
        "gc.collect()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr2hS673wtoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx['<pad>'] = 0\n",
        "word2idx['<unk>'] = 1\n",
        "idx = 2\n",
        "tokens = []\n",
        "max_len = 0\n",
        "for sent in texts:\n",
        "    _sent = word_tokenize(sent)\n",
        "    tokens.append(_sent)\n",
        "    max_len = max(len(_sent), max_len)\n",
        "    for word in _sent:\n",
        "        if word not in word2idx:\n",
        "            word2idx[word] = idx\n",
        "            idx += 1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEencuD0xNyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vecs = []\n",
        "for i in tokens:\n",
        "    i.extend(['<pad>'] * (max_len - len(i)))\n",
        "    vecs.append([word2idx[word] for word in i ])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y-ILtmSxQE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb = torch.tensor([model_ft.get_word_vector(i) for i in word2idx.keys()])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4RWvKbCxR8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rule = dict(enumerate(set(labels)))\n",
        "rule = {ii:i for i, ii in rule.items()}\n",
        "\n",
        "labels = [rule[i] for i in labels]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KImahrfxZxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
        "                              SequentialSampler)\n",
        "\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
        "    vecs, labels, test_size=0.1, random_state=42)\n",
        "\n",
        "    \n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "\n",
        "## обертка датасета (провила подачи на обучение)\n",
        "train_data = TensorDataset(train_inputs, train_labels)\n",
        "train_sampler = RandomSampler(train_data) # как выбираются батчи\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler)\n",
        "\n",
        "# Create DataLoader for validation data\n",
        "val_data = TensorDataset(val_inputs, val_labels)\n",
        "val_sampler = SequentialSampler(val_data) # как подаются\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYuL_349xn94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0Lt33FYxqZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 pretrained_embedding=None,\n",
        "                 freeze_embedding=False,\n",
        "                 vocab_size=None,\n",
        "                 embed_dim=300,\n",
        "                 num_classes=len(set(labels)),\n",
        "                 dropout=0.5):\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "        self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
        "                                                          freeze=freeze_embedding)\n",
        "                                                        \n",
        "        self.conv1d1 = nn.Conv1d(in_channels=self.embed_dim, # 300\n",
        "                                 out_channels=100,\n",
        "                                 kernel_size=3) \n",
        "\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.fc = nn.Linear(100, num_classes) ## ex-sum(num_filters)\n",
        "        self.dropout = nn.Dropout(p=dropout) ## overfitting\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "\n",
        "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "\n",
        "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
        "        # Output shape: (b, embed_dim, max_len)\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "\n",
        "       \n",
        "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
        "        x_conv = F.relu(self.conv1d1(x_reshaped)) \n",
        "\n",
        "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
        "        x_pool = F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "\n",
        "        ## remove second dimension \n",
        "        x_squz = x_pool.squeeze(dim=2)\n",
        "        \n",
        "        # Compute logits. Output shape: (b, n_classes)\n",
        "        logits = self.fc(self.dropout(x_squz))\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igiwGziuxvZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IyqfngKx0X5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "664e7c8a-c6ea-48d4-aecc-1473bbdbc16b"
      },
      "source": [
        "import torch.optim as optim\n",
        "import time \n",
        "import random\n",
        "m = CNN(pretrained_embedding=emb,\n",
        "                 freeze_embedding=False,\n",
        "                 vocab_size=len(word2idx),\n",
        "                 embed_dim=300,\n",
        "                 num_classes=len(set(labels)),\n",
        "                 dropout=0.5)\n",
        "\n",
        "m.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss() \n",
        "o = optim.Adam(m.parameters(), lr=0.001)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch_i in range(30):\n",
        "\n",
        "    t0_epoch = time.time()\n",
        "    total_loss = 0\n",
        "   \n",
        "    m.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        m.zero_grad()\n",
        "\n",
        "        logits = m(b_input_ids)\n",
        "\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        o.step()\n",
        "\n",
        "    # Calculate the average loss over the entire training data\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    m.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = m(b_input_ids)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "\n",
        "    # Track the best accuracy\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "\n",
        "    # Print performance over the entire training data\n",
        "    time_elapsed = time.time() - t0_epoch\n",
        "    print('epoch :', epoch_i, 'total loss: ', total_loss)\n",
        "    print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch : 0 total loss:  92693.46712614033\n",
            "   1    |   2.393139   |  2.282082  |   26.77   |  152.51  \n",
            "epoch : 1 total loss:  91758.62284770737\n",
            "   2    |   2.369004   |  2.285797  |   27.42   |  151.89  \n",
            "epoch : 2 total loss:  92200.38801175753\n",
            "   3    |   2.380409   |  2.320092  |   27.95   |  152.03  \n",
            "epoch : 3 total loss:  92148.46855435043\n",
            "   4    |   2.379069   |  2.302074  |   28.88   |  152.03  \n",
            "epoch : 4 total loss:  91700.35843872729\n",
            "   5    |   2.367500   |  2.409369  |   25.30   |  152.01  \n",
            "epoch : 5 total loss:  91956.44772426567\n",
            "   6    |   2.374111   |  2.386781  |   27.14   |  151.80  \n",
            "epoch : 6 total loss:  92238.39589268557\n",
            "   7    |   2.381390   |  2.375813  |   27.25   |  151.78  \n",
            "epoch : 7 total loss:  93068.06209074115\n",
            "   8    |   2.402811   |  2.438652  |   26.23   |  151.63  \n",
            "epoch : 8 total loss:  93068.82366311371\n",
            "   9    |   2.402830   |  2.553468  |   25.77   |  151.86  \n",
            "epoch : 9 total loss:  92976.59273431583\n",
            "  10    |   2.400449   |  2.669631  |   25.91   |  151.92  \n",
            "epoch : 10 total loss:  93047.6410310101\n",
            "  11    |   2.402283   |  2.659403  |   28.00   |  152.59  \n",
            "epoch : 11 total loss:  93300.3894416768\n",
            "  12    |   2.408809   |  2.672967  |   25.39   |  151.90  \n",
            "epoch : 12 total loss:  94544.60416779207\n",
            "  13    |   2.440932   |  2.728690  |   27.86   |  151.74  \n",
            "epoch : 13 total loss:  94261.71491938857\n",
            "  14    |   2.433628   |  2.668023  |   25.98   |  151.47  \n",
            "epoch : 14 total loss:  93698.41756420497\n",
            "  15    |   2.419085   |  2.802964  |   24.40   |  151.52  \n",
            "epoch : 15 total loss:  93912.80303307723\n",
            "  16    |   2.424620   |  2.712356  |   25.28   |  151.42  \n",
            "epoch : 16 total loss:  94191.4676559231\n",
            "  17    |   2.431814   |  2.884727  |   26.88   |  151.43  \n",
            "epoch : 17 total loss:  93363.87773481167\n",
            "  18    |   2.410448   |  2.888526  |   25.88   |  151.32  \n",
            "epoch : 18 total loss:  93357.00763217546\n",
            "  19    |   2.410271   |  2.762299  |   26.14   |  151.54  \n",
            "epoch : 19 total loss:  93708.4155307948\n",
            "  20    |   2.419343   |  2.999780  |   25.60   |  151.34  \n",
            "epoch : 20 total loss:  93323.70835746253\n",
            "  21    |   2.409411   |  2.921841  |   24.72   |  151.54  \n",
            "epoch : 21 total loss:  92992.13016217975\n",
            "  22    |   2.400850   |  3.202500  |   25.95   |  151.36  \n",
            "epoch : 22 total loss:  93172.6434480633\n",
            "  23    |   2.405511   |  3.062979  |   24.79   |  151.41  \n",
            "epoch : 23 total loss:  92739.46295225856\n",
            "  24    |   2.394327   |  3.312390  |   25.28   |  151.46  \n",
            "epoch : 24 total loss:  93012.9358097277\n",
            "  25    |   2.401387   |  3.299548  |   25.35   |  151.34  \n",
            "epoch : 25 total loss:  92906.52862040226\n",
            "  26    |   2.398640   |  3.189324  |   25.81   |  151.52  \n",
            "epoch : 26 total loss:  93166.02343089612\n",
            "  27    |   2.405340   |  3.250560  |   25.42   |  151.41  \n",
            "epoch : 27 total loss:  92509.23880108053\n",
            "  28    |   2.388383   |  3.177949  |   24.14   |  151.50  \n",
            "epoch : 28 total loss:  93208.58340269039\n",
            "  29    |   2.406439   |  3.309926  |   21.98   |  151.36  \n",
            "epoch : 29 total loss:  92436.56259294969\n",
            "  30    |   2.386507   |  3.038208  |   24.14   |  151.55  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpVOb_utcodf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(m.state_dict(), \"model.pt\")"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}